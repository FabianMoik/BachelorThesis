%%%% Time-stamp: <2013-02-25 10:31:01 vk>

\chapter{Implementation - Structure of the Bot}
\label{cha:implementation}
%%% Introduction
In this chapter the architecture of both the test environment and the poker playing agents is described and the learning process of the poker agents is explained. There are some key components that distinguish a strong poker player from a weak one. A well defined \textit{betting strategy} both attuned to the mathematical principles such as \textit{hand strength (HS)} and \textit{hand potential (HP)} and to the interpretation of opponent's tendencies decides on the profitability of a poker player over the \pokerterm{long run} \cite{opp_modeling}. This chapter describes a possible way to implement and apply these concepts to create a poker playing agent, capable of understanding the situation on the table and assessing its hand strength versus opponents. \pagebreak

%%%%%%%%%%%%%%%%%% ARCHITECTURE %%%%%%%%%%%%%%%%%%
\section{Architecture of Testbed and Neural Network Agents}
\label{sec:architecture}
Although there are a handful open source poker testbeds on the internet, none of them has proven to be suitable to achieve the goal of this thesis. Some of them lack the needed flexibility to extract all necessary information for the bot out of the current game state, others are not designed to run tournament poker simulations but only \pokerterm{cash games} or other poker variants. Therefore in this thesis a testbed was crafted, which can be arbitrarily modified to the needs of the user. However, the main focus of this thesis was on the creation of a test environment that can simulate millions of poker tournaments in as little time as possible. At the same time it should provide our poker agents with all the needed information about the current game state and opponents at the table. The poker testbed was written in \textit{C++} due to little performance overhead at runtime and its speed and efficiency.
\subsection{Poker Testbed}
The basic structure of the poker test environment, referred to as \textit{testbed} in this context consists of following components:\\
\pagebreak
\begin{multicols}{2}
\begin{itemize}
\item[$\triangleright$] Game object
\item[$\triangleright$] Table object
\item[$\triangleright$] Dealer object
\item[$\triangleright$] Player object
\end{itemize}
\columnbreak
\begin{itemize}
\item[$\triangleright$] Artificial Intelligence (AI) object
\item[$\triangleright$] Deck object
\item[$\triangleright$] Card object
\item[$\triangleright$] Rules object
\end{itemize}
\end{multicols}

To run an arbitrary number of simulated tournaments with one generation of agents, a \textit{game} object is initialized. The \textit{rules} for the game, including the \pokerterm{Buy-In}, the total number of players, the maximum number of players per table and the \pokerterm{blind structure}, are then passed to the game object and all participating \textit{players} are added to the game. \par Depending on the number of players participating in the tournament a number of \textit{table} objects are created. The game object then distributes all players across the tables and places one \textit{dealer} on each table. Dealers are not participating in the tournament but their job is it to deal cards to the players, apply forced bets (\textit{Small Blind}, \textit{Big Blind} and \textit{Antes}), deal community cards, determine the winning player for each hand and split the pot accordingly. Each dealer holds a \textit{deck} object which consists of 52 unique cards, 13 cards for each suit. The \textit{cards} themselves are also objects and contain the information about their \pokerterm{suit} and the card's value. They also offer some convenience functions for representing the card's value in a readable format for the human. \par
Each player holds an \textit{AI} object and is assigned a unique ID. The dealer tells the player when to act and provides him with all the public information about the game state. The player's AI then decides on the basis of this information how it would act in this situation and the player executes this action. In case of a betting action the player selects an appropriate bet size according to a betting strategy and places it on the table. An in depth look into the architecture of the bot is provided in subsection \ref{subsec:nnagent}. \par
Figure \ref{fig:highlevelarch} shows the high level architecture of the described testbed. Boxes represent objects, list items within the box represent properties of the corresponding object. Bidirectional arrows indicate that information is exchanged in both directions between two objects. Dotted arrows zoom in on a property of an object. Rounded boxes represent a collection of properties of an object.
\par
\myfig{architecture.pdf}%% filename in figures folder
  {width=1\textwidth,height=1\textheight}%% maximum width/height, aspect ratio will be kept
  {High Level Architecture of Testbed.}%% caption
  {High Level Architecture of Testbed}%% optional (short) caption for table of figures
  {fig:highlevelarch}%% label
%%%%%%%%%%%%%%%%%% NN AGENTS %%%%%%%%%%%%%%%%%%%%%%
\subsection{Neural Network Agents}
\label{subsec:nnagent}
In the conducted test series of simulating hundreds of tournaments in a single population of poker agents, evolving neural networks were used to train the agents on the game of NL Hold'em poker. An agent in this population is represented by the \textit{player object}, described in \ref{sec:architecture}. The player object holds an \textit{AI object} which implements a neural network and returns the desired action to be taken, given the public information about the current game state. This public game state information is provided by the \textit{dealer object}.
\subsubsection{Structure of the Neural Network}
The agents to be examined are implemented as fully connected feed-forward neural networks with a network topology of \markred{40-22-3}, which corresponds to 40 \textit{input neurons} in the input layer, 22 \textit{hidden neurons} in the hidden layer and 3 \textit{output neurons} in the output layer \cite{ENN_garrett}. For the hidden layer a \textbf{sigmoid} activation function is used and a \textbf{softmax} activation is performed at the output layer. The three output neurons represent the so called \textbf{probability triple} (f, c, r), which specifies the probability distribution for the actions \textit{fold, check/call} or \textit{bet/raise} at the current state of the game \cite{review}. The full architecture of the neural network with all its input features can be seen in Figure \ref{fig:nn_arch}. The selection of input features was strongly influence by the work of Nicolai in \cite{evolutionary_methods} and are described in the sequel. 
\myfig{nn_schematics.pdf}%% filename in figures folder
  {width=1.1\textwidth,height=1.1\textheight}%% maximum width/height, aspect ratio will be kept
  {Architecture of the neural network with all the input features}%% caption
  {Architecture of the neural network with all the input features}%% optional (short) caption for table of figures
  {fig:nn_arch}%% label
%\begin{table}[]
%\begin{tabular}{|l||l|}
%\hline
%\multicolumn{1}{|c||}{Input Node} & \multicolumn{1}{c|}{Feature} \\ \hhline{|=|=|}
%1                           & Effective Hand Strength      \\ \hline
%2                           & Betting round      \\ \hline
%3                           & \markred{Number of betting turns}      \\ \hline
%4                           & Chip count                   \\ \hline
%5                           & Chips in pot                 \\ \hline
%6                           & Chips to call                \\ \hline
%7                           & Number of opponents          \\ \hline
%8                           & Position of hero             \\ \hline
%7-14                        & Chip count of all opponents  \\ \hline
%15-22                       & \markred{Opponent model}               \\ \hline
%\end{tabular}
%\centering
%\caption{Table of inputs (features) for the neural network}
%\label{table:feature_tab}
%\end{table}
\subsubsection{Effective Hand Strength (EHS)}
The EHS is an indicator for how likely a hand is winning at showdown considering the current hand strength but also the positive and negative hand potential \cite{evolutionary_methods}. In conjunction with other components the EHS should be used to help selecting a suitable betting action. A more detailed description of EHS can be found in Subsection \ref{subsubsec:ehs}.
\subsubsection{Betting round}
The second input for the neural network is the current betting round in the game. This can be PREFLOP, FLOP, TURN or RIVER. The betting round is an important property because a good preflop strategy varies from a good postflop strategy and therefore this property should not be withheld from the neural network.
\subsubsection{Chip count}
The next feature is the number of chips the acting agent has. This represents the stack of the player. 
\subsubsection{Chips in pot}
The fourth feature is the number of chips already in the pot, including all chips of previous betting rounds plus the number of chips betted by other agents in the current betting round. An agent may not be able to win all the chips in the pot if his chip count (stack) is smaller than a bet of his opponents in the current round. This means when an opponent makes a bet larger than the remaining chips in the agent's stack, the agent may only win the amount of chips that is covered by his stack. Hence the value of this feature is the effective pot size an agent can actually win when his hand is the strongest at showdown \cite{evolutionary_methods}.
\subsubsection{Chips to call}
The fifth feature is the number of chips an agent has to match in order to be allowed to continue in the hand. 
The ratio between the amount of chips a player has to bet to stay in the hand and the amount of chips in the pot is called \pokerterm{pot odds}. This is a commonly used tool in the poker community to calculate the needed winning percentage of an agent's hand to make the call a profitable play, irrespective of other factors \cite{evolutionary_methods}.
\subsubsection{Number of opponents}
This feature represents the number of opponents still involved in the hand.
\subsubsection{Number of players left in tournament}
The seventh feature represents the total number of players, which are still left in the tournament. This feature is important because a strong poker player changes his strategy slightly when there are less players in the tournament. The reason being, that it is more profitable to play a tighter style once approaching the money ranks of a tournament.
\subsubsection{Position of hero}
This feature is the relative position of the agent to the dealer. The dealer position is the most valuable position in poker because the action of only two more players will follow in an unopened pot. In general it is desirable to be in a \pokerterm{late position} because many players have already acted before you and hence more information is available for the agent in late position \cite{evolutionary_methods}.
\subsubsection{Chip count of all opponents}
In a \pokerterm{9-handed} multi table tournament there are at most 9 players on one table. Therefore this feature has 8 input nodes, one for each opponent on the table. The input is the number of chips an opponent has. Late in a tournament it will occur that hands are not always played 9-handed but with less then 9 players per hand. For this case the input for empty seats on the table is set to unknown and will be recognized by the neural network as an empty seat. The first node of this 8 nodes is always the opponent directly to the left of the agent, the second node is the opponent two seats to the left of the agent \cite{evolutionary_methods}. 
\subsubsection{Opponent model for all opponents}
The final 24 features are used to model opponent's playing tendencies. Opponent modeling is important in the game of poker, because once a player has observed some information about the playing style of his opponents, he can adapt his own playing style to this new information. While playing against opponents in a series of tournaments there is a lot of information to be gained about opponents. In this thesis the process of modeling tendencies of opponents focused on three fundamental statistical values. The first value describes the frequency of voluntarily putting money in the pot \textbf{(VPIP)} (also known as VP\$IP). It can be calculated according to Equation \ref{eq:vpip}.
\begin{equation}
\label{eq:vpip}
VPIP_{\%} = \frac{\#CALLS_{P} + \#RAISES_{P}}{\#HANDS_{P}} \cdot 100\%
\end{equation}
$\#CALLS_{P}$ represents the number of hands a player has called with before the flop, $\#RAISES_{P}$ represents the number of hands a player has raised with before the flop and $\#HANDS_{P}$ is the total number of hands a player has played before the flop.\par
Similar to the \textit{VPIP} value the second statistical value, namely \textbf{Preflop Raise (PFR)}, shows the frequency of a player raising with a hand pre-flop instead of calling, checking or folding with it and can be calculated with Equation \ref{eq:pfr}.
\begin{equation}
\label{eq:pfr}
PFR_{\%} = \frac{\#RAISES_{P}}{\#HANDS_{P}}  \cdot 100\%
\end{equation}
The third statistical value used for opponent modeling is called \textbf{Aggression Frequency (AFq)} and can be used as an indicator for the overall aggression of an opponent. 
\begin{equation}
\label{eq:afq}
AFq_{\%} = \frac{\#RAISES}{\#RAISES + \#CALLS + \#FOLDS}  \cdot 100\%
\end{equation}
$\#RAISES$ represent the total number of hands a player has raised with, $\#CALLS$ and $\#FOLDS$ represent the number of hands a player has called with or folded, respectively. \par
If those three values are considered individually, they offer little information for a poker player, but if they are seen as a whole they give a good indication about the playing style of an opponent. As a rule of thumb, the higher the gap between \textit{VPIP} and \textit{PFR} is the more passive a player is \cite{playing_style}. 
\subsection{Key Components to Achieve High Level Poker Skill}
Billings et al. \cite{challenge_of_poker} described five main requirements a poker playing agent needs to fulfill in order to be competitive with the best players in the world. All these components depend on each other and need to be adjusted when a certain situation requires it. The five requirements mentioned by Billings are:
\begin{itemize}
\item[$\triangleright$] Hand Strength
\item[$\triangleright$] Hand Potential
\item[$\triangleright$] Bluffing
\item[$\triangleright$] Unpredictability
\item[$\triangleright$] Opponent Modeling
\end{itemize}
\textbf{Hand strength} and \textbf{hand potential} are explained in detail in subsection \ref{subsubsec:hs}. In short HS and HP combined assesses the relative hand strength against opponents, ideally considering opponent-specific factors that would influence the probability of winning a hand \cite{challenge_of_poker}. \par
To achieve this requirement a slightly different approach than recommended in \cite[p. 208]{challenge_of_poker} was implemented in this study. Instead of considering opponent-specific tendencies while calculating the hand strength and hand potential, the effective hand strength was calculated under the assumption of playing against a random player, the reason being that opponent's tendencies are later modeled individually and given as input to the neural network which should be able to convert this information into meaning.\par
\textbf{Opponent modeling} as described by Billings et al. \cite[p. 208]{challenge_of_poker} tries to accumulate information about the playing style of an opponent. Later this information is applied to hand strength calculations and used to create likely probability distributions of opponent's hand ranges. \par
Again in our study opponent models are not exactly used in the same way as described but rather represent key opponent-specific tendencies which do not directly influence any calculations but rather serve as additional input information for the neural network, to find an optimal betting strategy against different opponent types. \par
\textbf{Unpredictability} can be achieved by altering the strategy in a given situation. This means that a player should sometimes play differently in a similar situation, to not allow opponents to generate a precise model of the strategy \cite{challenge_of_poker}. \par
This component is achieved in this thesis by following certain probability distributions when it comes to bet sizes and betting actions. A detailed description of the betting strategy follows in subsection \ref{subsec:betting}.\par
\textbf{Bluffing} is the last requirement mentioned by Billings et al. \cite{challenge_of_poker} and serves the purpose of sometimes winning pots with weak hands. By sometimes bluffing with weak hands we make opponents doubt our hand strength and later win more money on our strong hands against them. \par
The bluffing component was not explicitly design in our study, but ideally the neural network should discover the concept of bluffing on its own, by interpreting the given opponent models and its relative hand strength.
\section{Betting Strategy}
%%%%%%%%%%%%%%%%%% BETTING STRATEGY %%%%%%%%%%%%%%%%%%%%%%
Billings \textit{et al.} \cite[p.~210]{challenge_of_poker} emphasis that betting strategies for pre-flop play and post-flop play are considerably different and that \enquote{a relatively simple expert system is sufficient for competent play} in the pre-flop phase. Nonetheless for the conducted study no expert system for pre-flop betting decisions was used but an evolving neural network was provided with public game state information for all betting rounds. While sharing the same neural network for both pre-flop and post-flop play there is one significant difference in calculating the HS for pre-flop decisions compared to post-flop calculations.
%%%% %%%%%%%%% PREFLOP
\subsection{Preflop Strategy}
The state space in the pre-flop stage of a poker hand is relatively small compared to the post-flop state space. In total there are ${52\choose 2} = 1352$ initial hole card combinations pre-flop, but this translates to only 169 distinct hand types because some hands have the same strength pre-flop but not post-flop. For example \Ah\Kh and \Ad\Kd have the same strength pre-flop and therefore are categorized into one distinct hand type, namely Ace King \pokerterm{suited} (AKs) \cite{opp_master}. \par
For evaluating the hand strength of one agent against $N$ opponents a lookup table was created. It consists of $169 * 8 = 1352$ entries, 169 distinct hand types played against one to eight opponents. A \textit{Monte-Carlo Simulation} of one million poker hands was performed for each distinct hand type against each possible number of opponents. During those one million hands the agent's hole cards were ranked against all opponents ' hole cards and all wins, ties and losses for the agent were counted. An approximation of the expected win percentage was calculated with following formula:
\begin{equation}
\label{eq:hs}
HS =  \frac{(\#WINS + \frac{\#TIES}{2})}{\#WINS + \#TIES + \#LOSSES} \hspace{1cm} .
\end{equation}
The hand strength (HS in the formula) represents the chance of a hand beating a random hand under the assumption of seeing all 5 community cards and going to showdown. This approach does not take opponent models into consideration \cite{opp_master}. \par
%%%%%%% HAND RANKER
\subsubsection{Hand Ranker}
A hand ranker as the name suggest, ranks a given poker hand based on its relative value by taking 5 to 7 cards as input and returning a number. The higher the number returned by the hand ranker, the better the hand \cite{hand_eval}. There exists a variety of open source hand ranking algorithms on the internet but only a few of them were able to hold their own against the competition over the years. The reason why only a hand full of poker hand ranking algorithms are used nowadays is the need for speed. Writing a simple hand ranker is rather easy, but writing an efficient one that is able to rank millions of hands every second is not so trivial. Among the most famous hand ranking algorithms one especially excelled due to its speed. It was created by the \textit{TwoPlusTwo (TPT)} community and uses a look up table with 32487834 entries which translates to a file size of approximately 250MB \cite{hand_eval}. The TwoPlusTwo poker hand ranker is one of the fastest of its kind to date. Not only is it extremely fast but also able to rank 5-card, 6-card and 7-card poker hands. This becomes very handy when evaluating hands pre-flop, on the flop and on the turn. For this reason the TPT hand ranker was used in multiple hand strength calculations throughout this study to determine if a given hand beats an other hand. 
\subsubsection{Hand Strength}
\label{subsubsec:hs}
While a hand ranker assigns a value to a given hand, it can only tell if a certain hand is better than other hands, and not how likely this hand currently is to win against a random hand. This is where the hand strength calculation comes into play. A simple approach of achieving this is to enumerate all possible remaining hand combinations an opponent could hold and count the number of wins, ties and losses versus the agent's hand. Using Formula \ref{eq:hs} one gets the probability that the current hand beats a random opponent's hand at the current round of the poker game. This value is also called the \textit{raw hand strength (RHS)}. While this approximation in combination with Monte-Carlo simulations might be sufficiently accurate for pre-flop hand strength assessment it is certainly insufficient for post-flop situations, when viewed in a vacuum. Raw hand strength calculations disregard the future potential of a hand and only represent the probability of a hand beating a random hand if there were no cards to come \cite{opp_master}. To overcome this obstacle the \textit{Effective Hand Strength} algorithm was conceived by Billings et al. and first published in \cite{opp_modeling}.
%%%% POSTFLOP
\subsection{Postflop Strategy}
For evaluating the strength of a hand after the pre-flop stage, it is important to not only consider the raw hand strength but also the positive and negative hand potential. Both metrics combined form the so-called \textit{effective hand strength} \cite{pena}.
\subsubsection{Effective Hand Strength}
\label{subsubsec:ehs}
The raw hand strength combined with the positive and negative hand potentials yield a measurement for the relative hand strength against an opponent competing in the poker hand. In general the effective hand strength can be represented by the following formula, as Billings et al. described it in \cite[p. 216]{challenge_of_poker}:
\begin{equation}
\label{eq:ehs}
EHS = HS \times (1 - NPot) + (1-HS) \times PPot \hspace{1cm},
\end{equation}
where $NPot$ represents the negative pot potential, which stands for the probability of falling behind opponents when we currently have the best hand. Conversely $PPot$ stands for the positive hand potential, the probability of improving the hand when currently being behind the opponents. The EHS calculation can be generalized for $n$ opponents by simply raising the HS to the power of \textit{number of opponents} \cite{opp_modeling}. This yields:
\begin{equation}
\label{eq:ehs_n_opp}
EHS = HS^{n} \times (1 - NPot) + (1-HS^{n}) \times PPot \hspace{1cm}.
\end{equation}
Although Billings et al. do not recommend to generalize the EHS calculation we decided to do so in our study because the accumulated information about tendencies of opponents was not directly used to influence hand strength calculations but rather serve as additional input to the neural network which in turn should learn to apply these models on its own.

\subsubsection{Hand Potential}
\label{subsubsec:hp}
Hand potential calculations are used to account for future cards to come in a poker hand and to assess the possible impact of these cards on the current raw hand strength \cite{challenge_of_poker}\footnote{More details on hand potential calculations and associated algorithms can be found in \cite[p. 216-218]{challenge_of_poker}.}.  
Assuming the current poker hand is in the flop-stage, with three cards already dealt to the community board, then there are ${45\choose 2} = 990$ possible combinations of future turn and river cards for every possible hand an opponent might hold. That means if one would like to calculate the hand potential of a current hand against a random opponent that would equate to ${47\choose 2} \times {45\choose 2} = 1081 \times 990 = 1070190$ calculations for this situation. \par
Because such a number of calculations is too expensive for our training algorithm, the effective hand strength calculation was modified to approximate the EHS.
\subsubsection{Effective Hand Strength Approximation}
Instead of calculating the \textit{NPOT} and \textit{PPOT} explicitly, the EHS approximation contains the \textit{NPOT} and \textit{PPOT} implicitly. A Monte-Carlos simulation similar to the one explained in Subsection \ref{subsubsec:hs} was used to determine a sufficient approximation of the EHS. Given an agent is currently one the Flop, Turn or River a simulation of 1000 games against $n$ active opponents is executed. If the agent is on the Flop, two cards are randomly drawn to simulate a possible Turn and River outcome, if he is on the Turn only one card is drawn and if he is on the River no additional cards are drawn. All $n$ opponents, which have not yet folded their cards and are therefore still in the current poker hand, are assigned 2 random cards and then the number of ties, wins and losses of the agent vs all opponents is counted. This process is repeated a 1000 times before the counting results are evaluated with Formula \ref{eq:hs}.\par
This simulation yields an approximated error of $\pm4\%$ of the real hand strength at a given moment. This approximation is good enough for training the agents because an exact hand strength turned out to be not the single most important feature in order to develop a profitable strategy.  
%\markblue{ Implementation: sample all possible two-card combinations of our opponents and only simulate the next round. Meaning on the flop simulate the Turn card (45 possible cards over a sample of possible opponent holdings. On the turn simulate the 44 possible cards vs samples of oppenent holdings.}
\pagebreak
\subsection{Betting Strategy}
\label{subsec:betting}
When a neural network agent is prompted to execute an action he feeds the provided game information through his neural network and generates a \textit{probability triple} at the output layer. The three output neurons represent the three possible moves (fold, check/call, bet/raise) an agent can execute. If an agents wants to \textit{bet} or \textit{raise} his hand he also needs to provide a betting amount in order for the move to be valid. This is realized by specifying two threshold values which decide the betting amount. Once an agent decides to bet or raise his hand, the value at the output of the third neuron (which corresponds to the \textit{raise} action) is checked against the two thresholds ($t_1$ and $t_2$). An output value of less than $t_1$ results in a \textit{small} bet amount, a value between $t_1$ and $t_2$ results in a \textit{medium} bet amount and a value greater than $t_2$ results in a \textit{large} bet amount.\par
	In modern NL Texas Hold'em the main decider of a \textit{small, medium} and \textit{large} bet amount is the player's chip count (CC) \cite{evolutionary_methods}. In this thesis a \textit{small} bet follows a normal distribution with a mean of $0.3 \cdot CC$ and a standard deviation of $0.05 \cdot CC$, while a \textit{medium} bet follows a normal distribution with a mean of $0.45 \cdot CC$ and a standard deviation of $0.1 \cdot CC$. A \textit{large} bet is normally distributed with a mean of $0.8 \cdot CC$ and a standard deviation of $0.05 \cdot CC$.
\pagebreak
%%%%%%%%%%%%% NN %%%%%%%%%%%%%
\section{Training the NN-Agents}
Unlike other games such as \textit{chess}, \textit{checkers} or \textit{go}, \textit{poker} is a game of imperfect information. This means, that a player can not see all relevant  information at a given game state at all times. Because the game of poker involves hidden information and deception, players must be willing to take risks based on the information they have at the current state of the game \cite{nn_evolve}. Unlike other methods, which reduce the large decision space of NL Hold'em by transforming the game into a smaller abstract version in which they then approximate the optimal path and execute the result in the original game via a translation method, Nicolai and Hilderman \cite{nn_evolve} introduced a method, in which evolutionary algorithms can be used to train neural network agents to achieve reasonably good results in the game of NL Texas Hold'em poker. The proposed algorithm in their work is used as a guide in this thesis to train neural network agents through iterative play over a large number of generations by mimicking natural evolution \cite{nn_evolve}.
\subsection{Evolution Phase}
Evolutionary algorithms try to mimic the evolution process as it is know in biology. Agents in a population are given a task and try to achieve the best results possible. The performance of agents is then evaluated and the best among the population are chosen to reproduce offsprings for the next generation. Offsprings are almost identical to their parents, but with slight changes to their genetic material. These offsprings together with the best agents from the previous generation are then again given the same task and the whole process repeats. \cite{evolutionary_methods, nn_evolve}. \par
In case of our neural network agents, the population of the first generation consists of randomly weighted neural network agents. They are competing in a number of tournaments against each other. The best agents of a generation are then chosen to reproduce offsprings. Together with the best agents, all offsprings are then competing in another series of tournaments until after $N$ generations the process is stopped. \par
Figure \ref{fig:evol_code} shows the implementation of simulating a number of tournaments for each generation and selecting a number of elite players per generation to evolve the neural network agents. \par
\pagebreak
\begin{figure}
\begin{lstlisting}
define NUM_ELITE_PLAYERS 5;
define NUM_TOURNAMENTS_PER_GEN 200;
define MUTATION_LIKELIHOOD 0.10;
define NUM_GENERATIONS 1000

void evolvePlayers(std::vector<Player*> players, std::vector<Player*> hall_of_fame, std::vector<Player*> population) {
	for (int i = 0; i < NUM_GENERATIONS; i++) {

        /// EVALUATION PHASE
        Rules* rules = defineRules();

        std::vector<Player*> elite;
        elite = runOneGeneration(population, players, rules, NUM_ELITE_PLAYERS, NUM_TOURNAMENTS_PER_GEN, hall_of_fame);
        
        updateHallOfFame(population, elite_players, hall_of_fame);

        /// EVOLUTION PHASE
        evolvePlayers(players, elite, MUTATION_LIKELIHOOD);
	}
}
\end{lstlisting}
\caption{The evolution process to train poker agents.}
\label{fig:evol_code}
\end{figure}
This algorithm shows the procedure of selecting the fittest agents of each generation and creating offsprings by evolving these elite players. The algorithm takes as input a vector of \textit{players}, which represents all the randomly created initial neural network agents, another vector called \textit{hall\_of\_fame}, which is initially also filled with randomly created neural network agents and yet another vector called \textit{population}, which forms the initial playing population consisting of all players and hall of fame members. Line 10 initialized the rules of the game, including the blind structure, the number of players per table, the starting chip stack for each player and many more properties. Line 12 declares a vector of elite players which is filled in Line 13 after \textit{NUM\_TOURNAMENTS\_PER\_GEN} tournaments are played in the current generation. After all tournaments have concluded the \textit{elite} vector holds the \textit{NUM\_ELITE\_PLAYERS} best players of the current generation. The best players of a given generation are determined by a fitness function explained in Subsection \ref{subsec:fitness}. In Line 15 the hall of fame is updated by a procedure which will be described in the following subsection \ref{subsec:hof}. In Line 18 the vector of elite players is then passed to Figure \ref{fig:evoplayers}.\par
\begin{figure}
\begin{lstlisting}
void evolvePlayers(std::vector<Player*> players, const std::vector<Player*> elite_players) {
   
    std::discrete_distribution<int> custom_distribution {4,25,6,1,1,1,1};
    std::uniform_real_distribution<> uniform_real_distribution(0,1);
    std::normal_distribution<double> normal_distribution(0, 0.6);

    for (long p = elite_players.size(); p < players.size(); p++) {

        int parent_count = custom_distribution() + 1;

        if (parent_count > elite_players.size()) {
            parent_count = elite_players.size();
        }

        std::vector<Player *> parents = chooseParents(parent_count);
        std::vector<double> parents_evolution_weights = assignParentsWeights(parent_count);
        
        std::vector<double> child_weights = players(p)->getOutputWeights();

        for (int w = 0; w < child_weights.size(); w++) {
            double new_weight = 0.0;

            for (int par = 0; par < parents.size(); par++) {
                std::vector<double> parent_weights = parents.at(
                        par)->getOutputWeights();
                new_weight += parent_weights[w] * parents_evolution_weights[par];
            }

            double random_value = uniform_real_distribution();
            if (random_value < MUTATION_LIKELIHOOD) {
                double noise_value = normal_distribution();
                new_weight += noiseValue;
            }
            child_weights.at(w) = new_weight;
        }
       players.at(p)->setOutputWeights(child_weights);
    }
}
\end{lstlisting}
\caption{The creation of offsprings by evolving elite players of a generation.}
\label{fig:evoplayers}
\end{figure}
The \textit{evolvePlayers} function takes as input a vector of \textit{players}, which represents all agents of the current generation ordered by their overall fitness, and a vector called \textit{elite\_players} which holds the fittest players of the current generation. This function handles the creation of new neural network agents by evolving pre-selected elite players of a given generation.\par
Lines 7 through 37 loop through all non-elite agents of the current generation which will later represent the newly created agents. Offsprings can have zero to \textit{NUM\_ELITE\_PLAYERS} parents. The number of parents for each new child is determined in Line 9. The most likely parent count is \textit{two}, followed by \textit{three} and \textit{one}, but there is also a slight chance that the number of parents is larger than three.\par
Once the number of parents is determined, the parents for reproduction are chosen in Line 15. Each \textit{elite\_player} is equally likely to be chosen as a parent but can only be chosen once. Line 16 determines the influence of each parent on the reproduction. Each parent is assigned a weight between zero and one, while the sum of all weights equals one. Next, all connection weights of the neural network of a child are loaded into a vector in Line 18. Lines 20 through 35 loop over all weights of the child and update them. The updated weight of each neural network connection is a biased average of the weights of the neural network of the parents \cite{evolutionary_methods}. This is represented by the Lines 23 through 27.\par
To avoid getting easily stuck in local minima and to explore the decision space, mutation is applied to the connection weights with a certain \textit{MUTATION\_LIKELIHOOD}. The mutation is represented by adding normally distributed noise with a mean of 0 and a standard deviation of 0.6 to the new weight of the child. The last step, shown in Line 36, is to apply the newly created weights to the child.

%\markred{Briefly explain that the evolutionary algorithm is used to update the weigth of the NN agents after one generation has played X number of tournaments. We don't us the classic back propagation algorithm but rather use the concept of natural selection. Where random mutation guarantees that agents don't get stuck in local optima. ... }
%\markblue{Mention self-play somewhere}
\subsection{Fitness Function}
\label{subsec:fitness}
A mixed fitness function, consisting of three components is used in this thesis to evaluate the performance of evolved neural network agents. The choice of the three components was greatly influenced by the work of Nicolai in \cite{evolutionary_methods}. The first component is the \textit{average ranking} in a tournament and can be obtained by ranking agents after each tournament, accumulating the results and then averaging them after a set of tournaments was played. The second component represents the \textit{average number of hands won per tournament}. The third component of the mixed fitness function represents the \textit{mean money won per tournament}. The mean money won can be calculated with Equation \ref{eq:meanmoney}.
\begin{equation}
MeanMoney = \frac{MoneyWon}{HandsWon} - \frac{MoneyLost}{HandsLost}
\label{eq:meanmoney}
\end{equation}\\
$MoneyWon$ represents the amount of chips a player has won over a set of tournaments, likewise $MoneyLost$ represents the amount of chips a player has lost. $HandsWon$ and $HandsLost$ are the number of hands a player has won and lost in this set of tournaments, respectively \cite{evolutionary_methods}. 
While this fitness function is used to evaluate and train agents in a given generation, another fitness function is later used to further benchmark the performance of the best agents per generation. The mixed fitness function is designed in a way that it rewards aggressive play more than passive play, because in real life poker aggressive strategies have shown to be superior to passive strategies. However just based on the average ranking in a tournament one can not conclude that a lower average ranking also means that the strategy is more profitable. Profitability in a poker tournament is measured by the so-called \textit{Return of Investment (ROI)}. To calculate the ROI of poker tournaments the accumulated winnings of all tournaments minus the accumulated cost of all tournaments is divided by the cost of all tournaments \cite{roi}.
\begin{equation}
ROI_{\%} = \frac{Acc. Winnings - Acc. Cost}{Acc. Cost} \cdot 100
\end{equation}\par
The overall fitness of an agent can be calculated as a weighted sum of all three components:
\begin{equation}
\label{eq:overall_fitness}
OverallFitness = w_1 \times Ranking_{avg} + w_2 \times HandsWon_{avg} + w_3 \times MeanMoney \hspace{1cm} .
\end{equation}
All three components are first linearly mapped to a range between 0 and 1 so that the weights $w_1,w_2$ and $w_3$ represent a percentage importance. Furthermore while an increase of $HandsWon$ and $MeanMoney$ indicate an improvement, it does not for the $Ranking$ component. Therefore the linearly mapping was reversed for this component. 
\subsection{Hall of Fame}
\label{subsec:hof}
In \cite[p. 63]{evolutionary_methods} Nicolai explains the non-transitive nature of poker. While agents of generation $A$ might defeat agents from generation $B$ frequently and agents from $B$ regularly defeat agents from generation $C$, it doesn't automatically follow that agents from $A$ also defeat agents from $C$ frequently. This means that there is no guarantee that an evolutionary system improves with each generation. \par
To counter this problem inherent in evolutionary algorithms a measure called \textit{Hall of Fame (HOF)} is implemented in this thesis. Successful strategies of previous generations are stored in the HOF and serve as a benchmark for newly created generations \cite{evolutionary_methods}. Together with players of the current generation, hall of fame members compete in a series of tournaments. All players are then ranked according to their overall fitness but members of the hall of fame and agents of the current generation are sorted individually. This means that all members of the HOF are sorted by their relative ranking in this generation and likewise agents of the current generation which are not members of the HOF are sorted relatively to their overall fitness. \par
The worst $\#x$ players of the HOF are then replaced by the best $\#x$ players of the current generation.
In Figure \ref{fig:hof} an exemplary representation of the individual sorting of HOF members and agents of the current generation is shown. In this example the number of possible replacements is chosen to be two. That means that the two best agents of the current generation replace the two worst members of the HOF because their overall fitness is better. If the overall fitness of the two best agents of the current generation would have been worse than the overall fitness of the worst HOF member, no replacement would have occurred.
 \myfig{hof.pdf}%% filename in figures folder
  {width=1\textwidth,height=1\textheight}%% maximum width/height, aspect ratio will be kept
  {Example of Hall of Fame member replacement.}%% caption
  {Example of Hall of Fame member replacement.}%% optional (short) caption for table of figures
  {fig:hof}%% label
  The new HOF is then used to serve as a benchmark for the next generation of newly evolved agents.
 HOF members are not involved in the evolution process, only agents of the current generation are.
\pagebreak

%\begin{itemize}
%\item introduction
%\item evolution phase
%\item selecting a fitness function
%\item hall of fame
%\item duplicat tables?
%\end{itemize}
%\begin{lstlisting}
%// Place pseudo code here
%\end{lstlisting}
%
%\begin{itemize}
%\item introduction
%\subitem outlook to what follows now
%\subitem mention that limitations were found and that the chapter describes how the limitations are or can be tackled.
%\item Neural Network Agent
%\subitem betting decision made according to the probability triple returned by the NN agents.
%\subsubitem explain how the action is choosen (bet size curves etc.)
%\item Training the Poker Agent
%\subitem Evolutionary algorithm
%\subitem Hall of Fame
%\subitem Co-evolution?
%\end{itemize}
%\glsresetall %% all glossary entries should be used in long form (again)
%% vim:foldmethod=expr
%% vim:fde=getline(v\:lnum)=~'^%%%%\ .\\+'?'>1'\:'='
%%% Local Variables:
%%% mode: latex
%%% mode: auto-fill
%%% mode: flyspell
%%% eval: (ispell-change-dictionary "en_US")
%%% TeX-master: "main"
%%% End:
